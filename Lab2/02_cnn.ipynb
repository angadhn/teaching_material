{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll use\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, csv\n",
    "\n",
    "# librosa is a widely-used audio processing library\n",
    "import librosa\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "# for accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# for data normalization\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION\n",
    "# Please alter the paths here to where the data are stored on your local filesystem\n",
    "binarylabelcsv  = os.path.expanduser(\"~/shared_storage/ECS7013P/bird_audio_detection/warblrb10k_public_metadata_2018.csv\")\n",
    "audiofilefolder = os.path.expanduser(\"~/shared_storage/ECS7013P/warblrb10k_public_wav\")\n",
    "\n",
    "# we experiment with 100 files here. In practice, it depends on your actual training, validation, and test data\n",
    "#maxfilestoload  = 1000      # limit, because loading the whole dataset is very slow\n",
    "maxfilestoload  = 100      # limit, because loading the whole dataset is very slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load the metadata labels\n",
    "binarylabels = {}\n",
    "with open(binarylabelcsv, 'r') as infp:\n",
    "        rdr = csv.DictReader(infp)\n",
    "        for row in rdr:\n",
    "                binarylabels[row['itemid']] = float(row['hasbird'])\n",
    "                if len(binarylabels)==maxfilestoload:\n",
    "                        break  # note, here we are restricting the maximum number of rows.\n",
    "\n",
    "fkeys = sorted(binarylabels.keys())\n",
    "# inspect:\n",
    "for i, kv in enumerate(binarylabels.items()):\n",
    "    print(kv)\n",
    "    if i==10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Load an example audio file, converting the audio data to mel spectrogram\n",
    "- window length 50 ms, hop_len 25 ms\n",
    "'''\n",
    "def extract_melspectrogram(filename, win_len=0.05, hop_len=0.025, n_mels=64):\n",
    "    audio, sr = librosa.load(\"%s/%s.wav\" % (audiofilefolder, filename), sr=22050)\n",
    "    win_len = int(win_len*sr)\n",
    "    hop_len = int(hop_len*sr)\n",
    "    spec = librosa.feature.melspectrogram(audio, sr, n_mels=n_mels, n_fft=2048, win_length=win_len, hop_length=hop_len)\n",
    "    # return data format (time_len, n_mels)\n",
    "    return spec.transpose((1,0))\n",
    "'''\n",
    " - Load the data, \n",
    " - Extract mel spectrograms\n",
    " - Annotation: one element corresponding to one audio file\n",
    "'''\n",
    "data = np.zeros((maxfilestoload, 400, 64)) # for storing mel spectrograms\n",
    "label = np.zeros(maxfilestoload) # for storing the annotion\n",
    "for i, kv in enumerate(binarylabels.items()):\n",
    "    print(kv[0])\n",
    "    # the number of the melspectrograms' time frames varies a bit (due to some small differences in audio length)\n",
    "    # for simplicity, let's take a maximum of 400 time frames.\n",
    "    data[i] = extract_melspectrogram(kv[0])[:400] \n",
    "    label[i] = kv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Split the data into \n",
    "    training (80%)\n",
    "    validation (10%)\n",
    "    test (10%)\n",
    "'''\n",
    "#print(label)\n",
    "#print(data.shape)\n",
    "#print(data[0])\n",
    "\n",
    "# training data\n",
    "train_data = data[:int(0.8*maxfilestoload)]\n",
    "train_label = label[:int(0.8*maxfilestoload)]\n",
    "print(train_data.shape)\n",
    "\n",
    "# validation data\n",
    "valid_data = data[int(0.8*maxfilestoload):int(0.9*maxfilestoload)]\n",
    "valid_label = label[int(0.8*maxfilestoload):int(0.9*maxfilestoload)]\n",
    "print(valid_data.shape)\n",
    "\n",
    "# test data\n",
    "test_data = data[int(0.9*maxfilestoload):]\n",
    "test_label = label[int(0.9*maxfilestoload):]\n",
    "print(test_data.shape)\n",
    "\n",
    "#del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalisation\n",
    "scaler = StandardScaler()\n",
    "# compute normalisation parameters based on the training data \n",
    "# QUESTION: why do we reshape the data to (-1,64)?\n",
    "scaler.fit(train_data.reshape((-1,64)))\n",
    "print(scaler.mean_)\n",
    "\n",
    "# normalise the training data with the computed parameters\n",
    "train_data = scaler.transform(train_data.reshape((-1,64)))\n",
    "train_data = train_data.reshape((-1, 400, 64)) # reverse back to the original shape\n",
    "#print(train_data[0])\n",
    "\n",
    "# normalise the validation data with the computed parameters\n",
    "valid_data = scaler.transform(valid_data.reshape((-1,64)))\n",
    "valid_data = valid_data.reshape((-1, 400, 64)) # reverse back to the original shape\n",
    "#print(valid_data[0])\n",
    "\n",
    "# normalise the test data with the computed parameters\n",
    "test_data = scaler.transform(test_data.reshape((-1,64)))\n",
    "test_data = test_data.reshape((-1, 400, 64)) # reverse back to the original shape\n",
    "#print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \n",
    "    Ref: He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing \n",
    "    human-level performance on imagenet classification.\" Proceedings of the \n",
    "    IEEE international conference on computer vision. 2015.\n",
    "    \"\"\"\n",
    "    \n",
    "    if layer.weight.ndimension() == 4:\n",
    "        (n_out, n_in, height, width) = layer.weight.size()\n",
    "        n = n_in * height * width\n",
    "        \n",
    "    elif layer.weight.ndimension() == 2:\n",
    "        (n_out, n) = layer.weight.size()\n",
    "\n",
    "    std = math.sqrt(2. / n)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    \n",
    "    bn.weight.data.fill_(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    \"\"\"The CNN model\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "        # FILLING THE ... TO COMPLETE THE 1ST 2D CONV LAYER OF THE NETWORK GIVEN:\n",
    "        # - kernel size 5x5\n",
    "        # - the number of kernels: 64\n",
    "        # What is the value of in_channels here?\n",
    "        # Ref: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.conv1 = nn.Conv2d(in_channels= ..., \n",
    "                               out_channels= ...,\n",
    "                               kernel_size=..., \n",
    "                               bias=False)\n",
    "\n",
    "        # FILLING THE ... TO COMPLETE THE 2ST 2D CONV LAYER OF THE NETWORK GIVEN:\n",
    "        # - kernel size 5x5\n",
    "        # - the number of kernels: 128\n",
    "        # What is the value of in_channels here?\n",
    "        self.conv2 = nn.Conv2d(in_channels=..., \n",
    "                               out_channels=...,\n",
    "                               kernel_size=..., \n",
    "                               bias=False)\n",
    "\n",
    "        # FILLING THE ... TO COMPLETE THE 3ST 2D CONV LAYER OF THE NETWORK GIVEN:\n",
    "        # - kernel size 3x3\n",
    "        # - the number of kernels: 128\n",
    "        # What is the value of in_channels here?\n",
    "        self.conv3 = nn.Conv2d(in_channels=..., \n",
    "                               out_channels=...,\n",
    "                               kernel_size=..., \n",
    "                               bias=False)\n",
    "\n",
    "        # FILLING THE ... TO COMPLETE THE FOLLOWING FULLY CONNECTED (DENSE) LAYER OF THE NETWORK GIVEN:\n",
    "        # - the number of hidden units: 128\n",
    "        # What is the value of in_features here? Hint, you need to work out the number of features after the last convolutional layer\n",
    "        # Ref: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fc1 = nn.Linear(..., \n",
    "                             ..., \n",
    "                             bias=True)\n",
    "        self.fc2 = nn.Linear(128, 1, bias=True)\n",
    "\n",
    "        # batch normalisation layers\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # call to initialise the network's weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_layer(self.conv3)\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_bn(self.bn3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (_, time_len, mel_bins) = x.shape\n",
    "        # reshape the input into 4D format (batch_size, channels, time_len, frequency_len)\n",
    "        x = x.view(-1, 1, time_len, mel_bins)\n",
    "        #print('Input')\n",
    "        #print(x.size())\n",
    "\n",
    "        # 1st conv layer + batch norm + relu activation\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # Note: the default stride is 1x1\n",
    "        x = nnF.relu(self.bn1(self.conv1(x)))\n",
    "        #print('Conv1')\n",
    "        #print(x.size())\n",
    "        \n",
    "        # max pooling with kernel size (8, 4)\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # QUESTION: WHAT IS THE EFFECT OF PADDING HERE?\n",
    "        x = nnF.max_pool2d(x,kernel_size=(8,4),padding=(4,0))\n",
    "        #print('Pool1')\n",
    "        #print(x.size())\n",
    "        \n",
    "        # 2nd conv layer + batch norm + relu activation\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # Note: the default stride is 1x1\n",
    "        x = nnF.relu(self.bn2(self.conv2(x)))\n",
    "        #print('Conv2')\n",
    "        #print(x.size())\n",
    "        \n",
    "        # max pooling with kernel size (8, 4)\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        x = nnF.max_pool2d(x,kernel_size=(8,4),padding=(2,1))\n",
    "        #print('Pool2')\n",
    "        #print(x.size())\n",
    "        \n",
    "        # 3rd conv layer + batch norm + relu activation\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # Note: the default stride is 1x1\n",
    "        x = nnF.relu(self.bn3(self.conv3(x)))\n",
    "        #print('Conv3')\n",
    "        #print(x.size())\n",
    "        \n",
    "        # max pooling with kernel size (8, 4)\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        x = nnF.max_pool2d(x,kernel_size=(2,1))\n",
    "        #print('Pool3')\n",
    "        #print(x.size())\n",
    "        \n",
    "        # flatten the feature map into a vector\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # the first dense layer + relu activation\n",
    "        x = nnF.relu(self.fc1(x))\n",
    "        # the first dense layer + sigmoid activation\n",
    "        # QUESTION: WHY DO WE NEED TO USE SIGMOID ACTIVATION HERE?\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_and_convert(self, x):\n",
    "        \"Handles the torch<--->numpy tensor conversion, for convenience\"\n",
    "        x_torch = torch.FloatTensor(x)\n",
    "        y_torch = self.forward(x_torch)\n",
    "        return y_torch.detach().numpy()\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model instance\n",
    "net = CnnModel()\n",
    "print(net)\n",
    "\n",
    "# Binary-cross entropy loss, closely related to logistic regression loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Adam Optimizer, learning rate 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch size (remember stochastic gradient descent?)\n",
    "batch_size = 4\n",
    "\n",
    "# some helpful functions\n",
    "\n",
    "'''\n",
    "Evaluate a network \"model\" on the data \"data\" \n",
    "Predicted class labels will be returned\n",
    "'''\n",
    "def evaluate(model, data):\n",
    "    pred = np.zeros(len(data)) # for storing predicted class labels, one for each data sample\n",
    "    num_batch = len(data)//batch_size # number of batches in one data epoch\n",
    "    # evaluate batch by batch and store the output to \"pred\"\n",
    "    for i in range(num_batch):\n",
    "        temp = model.forward_and_convert(data[i*num_batch : (i+1)*num_batch])\n",
    "        pred[i*num_batch : (i+1)*num_batch] = temp.squeeze()\n",
    "    # some trailing data samples\n",
    "    if(num_batch*batch_size < len(data)):\n",
    "        temp = model.forward_and_convert(data[num_batch*batch_size :])\n",
    "        pred[num_batch*batch_size :] = temp.squeeze()\n",
    "    # each element in \"pred\" is the output after sigmoid function and has value in [0, 1].\n",
    "    # to obtain the discrete label (0 or 1 in this case), we threshold the value by 0.5.\n",
    "    pred[pred >= 0.5] = 1.\n",
    "    pred[pred < 0.5] = 0.\n",
    "    return pred\n",
    "\n",
    "'''\n",
    "Randomly shuffle the data. It will be used to shuffle the training data after every training epoch\n",
    "'''\n",
    "def shuffle_data(data, label):\n",
    "    # permute the data indices\n",
    "    rand_ind = np.random.permutation(len(data))\n",
    "    # re-order the data with the pumuted indices\n",
    "    return data[rand_ind], label[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The training loop'''\n",
    "\n",
    "num_epochs = 100 # the number of training epoch (i.e. when you've gone through all samples of the training data, that's one epoch)\n",
    "evaluate_every_epoch = 1 # how often you want to evaluate the network during training?\n",
    "best_valid_acc = 0.0 # for keeping track of the best accuracy on the validation data\n",
    "saved_model = './best_model' # path for saving the best model during training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle training data\n",
    "    train_data, train_label = shuffle_data(train_data, train_label)\n",
    "    \n",
    "    # the number of minibatch in one epoch\n",
    "    num_batch = len(train_data) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        # sample one minibatch\n",
    "        # FILLING ... TO COMPLETE THE LINES BELOW TO SAMPLE THE I-TH MINIBATCH OF DATA FOR TRAINING\n",
    "        # Hint: you need to think about the starting and ending index of this minibatch in 'train_data' and 'train_label'\n",
    "        batch_data = train_data[...]\n",
    "        label_data = train_label[...]\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(torch.FloatTensor(batch_data))\n",
    "        loss = criterion(outputs.squeeze(), torch.FloatTensor(label_data))\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss = loss.item()\n",
    "    # print training loss\n",
    "    print('[%d] loss: %.8f' %(epoch, running_loss))\n",
    "    \n",
    "    # evaluate the network on the validation data\n",
    "    if((epoch+1) % evaluate_every_epoch == 0):\n",
    "        valid_pred = evaluate(net, valid_data)\n",
    "        valid_acc = accuracy_score(test_pred, valid_label)\n",
    "        print('Validation accuracy: %g' % valid_acc)\n",
    "        \n",
    "        # if the best validation performance so far, save the network to file \n",
    "        if(best_valid_acc < acc):\n",
    "            print('Saving best model')\n",
    "            # COMPLETE THE LINE BELOW TO SAVE THE CURRENT BEST MODEL.\n",
    "            # - the path of the model is in the variable 'saved_model'\n",
    "            # Ref: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''When you are here, we have the best model saved in file.'''\n",
    "'''Then, load the saved model, and evaluate it on the test data'''\n",
    "net = CnnModel()\n",
    "# COMPLETE THE LINE BELOW TO LOAD THE SAVED MODEL.\n",
    "# - the path of the model is in the variable 'saved_model'\n",
    "# Ref: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "...\n",
    "\n",
    "# evaluate on the test data\n",
    "test_pred = evaluate(net, test_data) \n",
    "print(test_pred)\n",
    "\n",
    "# test accuracy\n",
    "test_acc = accuracy_score(test_pred, test_label)\n",
    "print('Test accuracy: %g' % test_acc)\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix(test_label, test_pred)\n",
    "print('Confusion_matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
